import 'package:ml_algo/src/cost_function/cost_function.dart';
import 'package:ml_algo/src/linear_optimizer/gradient_optimizer/gradient_optimizer.dart';
import 'package:ml_algo/src/linear_optimizer/initial_coefficients_generator/initial_coefficients_type.dart';
import 'package:ml_linalg/dtype.dart';
import 'package:ml_linalg/linalg.dart';
import 'package:mockito/mockito.dart';
import 'package:test/test.dart';

import '../../mocks.mocks.dart';

void main() {
  group('Gradient optimizer', () {
    final costFunctionMock = MockCostFunction();
    late MockRandomizer randomizerMock;
    late MockInitialCoefficientsGenerator initialCoefficientsGeneratorMock;

    final points = Matrix.fromList([
      [5, 10, 15],
      [1, 2, 3],
      [10, 20, 30],
      [100, 200, 300],
    ]);
    final labels = Matrix.column([10.0, 20.0, 30.0, 40.0]);
    final defaultIterationsCount = 3;
    final providedCoefficients = Matrix.column([1.5, 2.5, 3.5]);
    final autoGeneratedInitialCoefficients = Matrix.column([10.5, -2.5, 13.75]);
    final interval = [0, points.rowsNum];
    final learningRate = 10.0;
    final gradient = Matrix.column([100, 200, 300]);
    final batchSize1 = 1;
    final batchSize2 = 2;
    final bigBatchSize = 1000;
    final negativeBatchSize = -1;
    final zeroBatchSize = 0;
    final defaultLambda = 105.1;
    final cost = 12009.23;
    final defaultLearningRates = [learningRate, learningRate, learningRate];
    final minCoefficientsUpdate = 0.001;

    final providedCoeffsIteration2 =
        providedCoefficients - gradient * learningRate;
    final providedCoeffsIteration3 =
        providedCoeffsIteration2 - gradient * learningRate;

    final autoGenCoeffsIteration2 =
        autoGeneratedInitialCoefficients - gradient * learningRate;
    final autoGenCoeffsIteration3 =
        autoGenCoeffsIteration2 - gradient * learningRate;

    final createGradientOptimizer = ({
      required int batchSize,
      CostFunction? costFunction,
      InitialCoefficientsType? initialCoefficientsType,
      Iterable<double>? learningRates,
      double? decay,
      double? lambda,
      int? randomSeed,
      DType? dtype,
    }) =>
        GradientOptimizer(
          points,
          labels,
          costFunction: costFunction ?? costFunctionMock,
          batchSize: batchSize,
          dtype: dtype ?? DType.float32,
          lambda: lambda ?? defaultLambda,
          randomizer: randomizerMock,
          learningRates: learningRates ?? defaultLearningRates,
          initialCoefficientsGenerator: initialCoefficientsGeneratorMock,
          minCoefficientsUpdate: minCoefficientsUpdate,
        );

    setUp(() {
      randomizerMock = MockRandomizer();
      initialCoefficientsGeneratorMock = MockInitialCoefficientsGenerator();

      when(
        initialCoefficientsGeneratorMock.generate(
          argThat(anything),
        ),
      ).thenReturn(
        autoGeneratedInitialCoefficients.toVector(),
      );

      when(
        randomizerMock.getIntegerInterval(
          argThat(anything),
          argThat(anything),
          intervalLength: anyNamed('intervalLength'),
        ),
      ).thenReturn(interval);

      when(
        costFunctionMock.getCost(
          argThat(anything),
          argThat(anything),
          argThat(anything),
        ),
      ).thenReturn(cost);

      when(
        costFunctionMock.getGradient(
          argThat(anything),
          argThat(anything),
          argThat(anything),
        ),
      ).thenReturn(gradient);
    });

    tearDown(() {
      reset(initialCoefficientsGeneratorMock);
      reset(randomizerMock);
      reset(costFunctionMock);
    });

    test('should process `batchSize` parameter, batchSize=$batchSize1', () {
      final optimizer = createGradientOptimizer(
        batchSize: batchSize1,
      );

      optimizer.findExtrema();

      verify(
        randomizerMock.getIntegerInterval(
          argThat(anything),
          argThat(anything),
          intervalLength: batchSize1,
        ),
      ).called(defaultIterationsCount);
    });

    test('should process `batchSize` parameter, batchSize=$batchSize2', () {
      final optimizer = createGradientOptimizer(
        batchSize: batchSize2,
      );

      optimizer.findExtrema();

      verify(
        randomizerMock.getIntegerInterval(
          argThat(anything),
          argThat(anything),
          intervalLength: batchSize2,
        ),
      ).called(defaultIterationsCount);
    });

    test('should throw an exception if too big batch size is provided', () {
      final actual = () => createGradientOptimizer(
            batchSize: bigBatchSize,
          );

      expect(actual, throwsRangeError);
    });

    test('should throw an exception if negative batch size is provided', () {
      final actual = () => createGradientOptimizer(
            batchSize: negativeBatchSize,
          );

      expect(actual, throwsRangeError);
    });

    test('should throw an exception if zero batch size is provided', () {
      final actual = () => createGradientOptimizer(
            batchSize: zeroBatchSize,
          );

      expect(actual, throwsRangeError);
    });

    test(
        'should consider coefficients diff while the convergence is being '
        'detected', () {
      final optimizer = createGradientOptimizer(
        batchSize: batchSize1,
        lambda: 0,
      );
      final expectedDiff =
          ((autoGeneratedInitialCoefficients - gradient * learningRate) -
                  autoGeneratedInitialCoefficients)
              .norm();

      optimizer.findExtrema();
    });

    test('should calculate gradient using coefficients, points and labels', () {
      final optimizer = createGradientOptimizer(
        batchSize: batchSize1,
        lambda: 0,
      );

      optimizer.findExtrema();

      verify(
        costFunctionMock.getGradient(
          points,
          autoGeneratedInitialCoefficients,
          labels,
        ),
      ).called(1);

      verify(
        costFunctionMock.getGradient(points, autoGenCoeffsIteration2, labels),
      ).called(1);

      verify(
        costFunctionMock.getGradient(
          points,
          autoGenCoeffsIteration3,
          labels,
        ),
      ).called(1);
    });

    test(
        'should calculate gradient using coefficients, points and labels, '
        'initial coefficients are provided', () {
      final optimizer = createGradientOptimizer(
        batchSize: batchSize1,
        lambda: 0,
      );

      optimizer.findExtrema(initialCoefficients: providedCoefficients);

      verify(costFunctionMock.getGradient(
        points,
        providedCoefficients,
        labels,
      )).called(1);

      verify(costFunctionMock.getGradient(
        points,
        providedCoeffsIteration2,
        labels,
      )).called(1);

      verify(
        costFunctionMock.getGradient(
          points,
          providedCoeffsIteration3,
          labels,
        ),
      ).called(1);
    });

    test('should regularize coefficients', () {
      final optimizer = createGradientOptimizer(
        batchSize: batchSize1,
      );
      final iteration2Coefficients =
          providedCoefficients * (1 - 2 * learningRate * defaultLambda) -
              gradient * learningRate;
      final iteration3Coefficients =
          iteration2Coefficients * (1 - 2 * learningRate * defaultLambda) -
              gradient * learningRate;

      optimizer.findExtrema(initialCoefficients: providedCoefficients);

      verify(
        costFunctionMock.getGradient(
          points,
          providedCoefficients,
          labels,
        ),
      ).called(1);

      verify(
        costFunctionMock.getGradient(
          points,
          iteration2Coefficients,
          labels,
        ),
      ).called(1);

      verify(
        costFunctionMock.getGradient(
          points,
          iteration3Coefficients,
          labels,
        ),
      ).called(1);
    });

    test('should calculate cost values if collectLearningData is true', () {
      final optimizer = createGradientOptimizer(
        batchSize: batchSize1,
        lambda: 0,
      );

      optimizer.findExtrema(collectLearningData: true);

      verify(
        costFunctionMock.getCost(
          points,
          autoGeneratedInitialCoefficients,
          labels,
        ),
      ).called(1);

      verify(
        costFunctionMock.getCost(
          points,
          autoGenCoeffsIteration2,
          labels,
        ),
      ).called(1);

      verify(
        costFunctionMock.getCost(
          points,
          autoGenCoeffsIteration3,
          labels,
        ),
      ).called(1);
    });

    test('should collect cost per iteration if collectLearningData is true',
        () {
      final optimizer = createGradientOptimizer(
        batchSize: batchSize1,
      );

      optimizer.findExtrema(collectLearningData: true);

      expect(optimizer.costPerIteration, [cost, cost, cost]);
    });

    test('should clear cost per iteration list if optimization starts over',
        () {
      final optimizer = createGradientOptimizer(
        batchSize: batchSize1,
      );

      optimizer.findExtrema(collectLearningData: true);
      expect(optimizer.costPerIteration, [cost, cost, cost]);

      optimizer.findExtrema();
      expect(optimizer.costPerIteration, <num>[]);
    });
  });
}
